{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Well Calibrated Illness Severity Scores\n",
    "## Model Building\n",
    "### C.V. Cosgriff, MIT Critical Data\n",
    "\n",
    "The goal of this notebook is to apply current state-of-the-art for predictive modeling with structured data to a full ICU cohort and high-risk sub-cohort in order to compare the  discriminative ability and calibration in the high-risk cohort. We seek to determine which strategy leads to models that can accurately forecast mortality in high-risk subsets where previous models have struggled by examining the role of constraining the case-severity mix.\n",
    "\n",
    "With respect to the modeling approaches, we'll implement a generalized linear model similar to APACHE IV as a baseline comparison, but, given the extremely large number of features we will impose regularization to reduce model complexity and prevent overfitting; we will therefore employ $L2$ penalization also known as ridge regression. We will also implement a tree based approach as these have been exceedingly successful in recent works and is considered the state-of-the-art for predictive modeling with structured data. Specifically, we will use a gradient boosted tree approach as implemented by the _extreme gradient boosting_ algorithim in `xgBoost` _(XGBoost: A Scalable Tree Boosting System, arXiv:1603.02754 [cs.LG])_.\n",
    "\n",
    "__Notebook Outline:__\n",
    "* Envrionment preparation\n",
    "* Load training data\n",
    "* Train models on full cohort\n",
    "    * Penalized generalized linear model ($L2$, _ridge regression_)\n",
    "    * Gradient boosted tree (_xgBoost_)\n",
    "* Train models on high-risk subset of full cohort\n",
    "    * Penalized generalized linear model ($L2$, _ridge regression_)\n",
    "    * Gradient boosted tree (_xgBoost_)\n",
    "    \n",
    "_All model (hyper-)parameters will be chosen by 5-fold cross validation._\n",
    "\n",
    "## 0 - Environment Setup\n",
    "\n",
    "Here we'll load the standard data science stack, preprocessing, linear model, and model evalutaion tools from `scikit-learn`, and gradient boosting classifier from `xgboost`. We are using version 0.19.2 of `scikit-learn` and version 0.72 of `xgboost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_full = pd.read_csv('../extraction/data/train_X.csv').set_index('patientunitstayid').values\n",
    "train_y_full = pd.read_csv('../extraction/data/train_y.csv').values.ravel()\n",
    "train_apache_full = pd.read_csv('../extraction/data/train_apache.csv').values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seperate off the high-risk subcohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_HR = train_X_full[(train_apache_full >= 0.10), :] \n",
    "train_y_HR = train_y_full[(train_apache_full >= 0.10)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the _Full Cohort_ will always have more data we'll randomly sample from the full training set so that the data are the same size of the _high-risk cohort_. These data will define the RS cohort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)\n",
    "sample_index = np.random.choice(np.arange(0, train_X_full.shape[0]), size=train_y_HR.shape[0])\n",
    "train_X = train_X_full[sample_index, :]\n",
    "train_y = train_y_full[sample_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - RS Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ridge Logistic Regression__\n",
    "\n",
    "Our first model is a linear model similar to that used in the development of APACHE IV. As we have many variables, we choose to constrain model complexity using a $L2$ regularization, and thus will train a ridge logistic regression model. For selection of $\\lambda$, which `scikit-learn` calls $\\frac{1}{C}$, we'll use 5-fold cross validation.\n",
    "\n",
    "Unlike tree based approaches, ridge regression requires features be of the same scale for proper performance. It is also not robust to missing data. As such, we'll employ mean imputation, followed by scaling and centering of the features, and then ridge regression with 5-fold CV searching for $\\lambda$ in  $[10, 5000)$ with a step-size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC estimated by 5-fold CV: 0.895\n",
      "Optimal lambda: 257.000\n"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "lam = np.arange(1, 500, 1)\n",
    "ridge_classifier = Pipeline([('impute', Imputer(strategy='median')),\n",
    "                             ('center_scale', RobustScaler()),\n",
    "                             ('ridge', LogisticRegressionCV(Cs=(1/lam), cv=K, \n",
    "                                                            scoring='roc_auc', \n",
    "                                                            n_jobs=4, refit=True, \n",
    "                                                            random_state=42))])\n",
    "ridge_classifier.fit(train_X, train_y)\n",
    "\n",
    "scores = ridge_classifier.named_steps['ridge'].scores_[1]\n",
    "scores_fold_avg = np.mean(scores, axis=0)\n",
    "print('Best AUC estimated by 5-fold CV: {0:.3f}'.format(scores.max()))\n",
    "print('Optimal lambda: {0:.3f}'.format(1 / ridge_classifier.named_steps['ridge'].C_[0]))\n",
    "\n",
    "pickle.dump(ridge_classifier, open('./models/ridge_full-cohort', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Boosting__  \n",
    "\n",
    "As compared to logistic regression, which directly minimizes the log-loss via MLE, boosted trees are typically poorly calibrated classifiers _(Obtaining Calibrated Probabilities from Boosting, arXiv:1207.1403 [cs.LG])_. However, `xgboost` implements multiple objective functions and can use the log-loss as logistic regression does. As such, Platt scaling and Isotonic Regression are unnecessary to achieve well-calibrated classifiers with a properly chosen loss function.\n",
    "\n",
    "In contrast to ridge regression, there are far more hyperparameters to choose from with the gradient boosting model, and thus an exhaustive search is very computationally expensive. Hyperparameters will therefore be obtained by a random sampling of the hyperparaemter space, as opposed to an exhaustive grid search. Bergstra et al. showed this to be superior whilst remaining computationally cheaper _(Journal of Machine Learning Research 13 (2012) 281-30)_. This will be implemented below and the grid will be sampled from 100 times. Our version of `xgboost` is compiled to use the GPU, and thus we'll use the GPU version of the fast histogram algorithim; the GPU on this machine is a Titan Xp. However, because of the implementation we can only run one job at a time, although this is still substantially faster than parallelizing over CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 35.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC estimated by 5-fold CV: 0.930\n"
     ]
    }
   ],
   "source": [
    "params = {'objective':['binary:logistic'],\n",
    "          'learning_rate': [0.01, 0.05, 0.10],\n",
    "          'max_depth': [3, 6, 9, 12],\n",
    "          'min_child_weight': [6, 8, 10, 12],\n",
    "          'silent': [True],\n",
    "          'subsample': [0.6, 0.8, 1],\n",
    "          'colsample_bytree': [0.5, 0.75, 1],\n",
    "          'n_estimators': [500, 1000]}\n",
    "\n",
    "xgb_model = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "cv_grid_search = RandomizedSearchCV(xgb_model, param_distributions=params, n_iter=100, \n",
    "                                    scoring='roc_auc', n_jobs=1, cv=skf.split(train_X, train_y), \n",
    "                                    verbose=1, random_state=42)\n",
    "cv_grid_search.fit(train_X, train_y)\n",
    "print('Best AUC estimated by 5-fold CV: {0:.3f}'.format(cv_grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then output the best model to examine the parameters chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.75, gamma=0, learning_rate=0.01,\n",
      "       max_delta_step=0, max_depth=12, min_child_weight=6, missing=None,\n",
      "       n_estimators=1000, n_jobs=1, nthread=None,\n",
      "       objective='binary:logistic', predictor='gpu_predictor',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.6, tree_method='gpu_hist')\n"
     ]
    }
   ],
   "source": [
    "print(cv_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit this optimal estimator on the full training set and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = cv_grid_search.best_estimator_\n",
    "xgb_classifier.fit(train_X, train_y)\n",
    "pickle.dump(xgb_classifier, open('./models/xgb_full-cohort', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - High-risk Models\n",
    "\n",
    "We'll now repeat the above modeling steps in the high-risk cohort. We begin by selecting the high-risk cohort, which was defined above as patients with an APACHE IV predicted mortality $\\geq0.10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will produce the same models as above. Because are only changing the training data, the process will be less verbose and unless otherwise stated everything is as above.\n",
    "\n",
    "__Ridge Logistic Regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC estimated by 5-fold CV: 0.823\n",
      "Optimal lambda: 425.000\n"
     ]
    }
   ],
   "source": [
    "ridge_classifier_HR = Pipeline([('impute', Imputer(strategy='median')),\n",
    "                                ('center_scale', RobustScaler()),\n",
    "                                ('ridge', LogisticRegressionCV(Cs=(1/lam), cv=K, \n",
    "                                                               scoring='roc_auc', \n",
    "                                                               n_jobs=4, refit=True, \n",
    "                                                               random_state=42))])\n",
    "ridge_classifier_HR.fit(train_X_HR, train_y_HR)\n",
    "\n",
    "scores = ridge_classifier_HR.named_steps['ridge'].scores_[1]\n",
    "scores_fold_avg = np.mean(scores, axis=0)\n",
    "print('Best AUC estimated by 5-fold CV: {0:.3f}'.format(scores.max()))\n",
    "print('Optimal lambda: {0:.3f}'.format(1 / ridge_classifier_HR.named_steps['ridge'].C_[0]))\n",
    "\n",
    "pickle.dump(ridge_classifier_HR, open('./models/ridge_HR-cohort', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Boosting__\n",
    "\n",
    "5-fold CV hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 43.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC estimated by 5-fold CV: 0.852\n"
     ]
    }
   ],
   "source": [
    "params = {'objective':['binary:logistic'],\n",
    "          'learning_rate': [0.01, 0.05, 0.10],\n",
    "          'max_depth': [3, 6, 9, 12],\n",
    "          'min_child_weight': [6, 8, 10, 12],\n",
    "          'silent': [True],\n",
    "          'subsample': [0.6, 0.8, 1],\n",
    "          'colsample_bytree': [0.5, 0.75, 1],\n",
    "          'n_estimators': [500, 1000]}\n",
    "\n",
    "xgb_model = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "cv_grid_search_HR = RandomizedSearchCV(xgb_model, param_distributions=params, n_iter=100, \n",
    "                                    scoring='roc_auc', n_jobs=1, cv=skf.split(train_X_HR, train_y_HR), \n",
    "                                    verbose=1, random_state=42)\n",
    "cv_grid_search_HR.fit(train_X_HR, train_y_HR)\n",
    "print('Best AUC estimated by 5-fold CV: {0:.3f}'.format(cv_grid_search_HR.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the optimal estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=9, min_child_weight=10, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
      "       predictor='gpu_predictor', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=0.6, tree_method='gpu_hist')\n"
     ]
    }
   ],
   "source": [
    "print(cv_grid_search_HR.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit optimal model on all training data, and save the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier_HR = cv_grid_search_HR.best_estimator_\n",
    "xgb_classifier_HR.fit(train_X_HR, train_y_HR)\n",
    "pickle.dump(xgb_classifier_HR, open('./models/xgb_HR-cohort', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to model analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
