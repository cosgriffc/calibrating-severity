{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Well Calibrated Illness Severity Scores\n",
    "## Model Analysis\n",
    "### C.V. Cosgriff, MIT Critical Data\n",
    "\n",
    "With our models developed, we will examine their discrimination and calibration. Discrimination will be examined via area under the reciever-operator characteristic curve (AUC). Calibration will be examined via reliability curves, and the Brier score, which captures both discrimination and calibration will be calculated. All metrics will have conservative 95% confidence intervals constructed via bootstrapping.\n",
    "\n",
    "__Notebook Outline:__\n",
    "* Envrionment preparation\n",
    "* Load held out test set\n",
    "* Model Evaluation\n",
    "    * Discrimination\n",
    "    * Calibration\n",
    "* Feature Analysis\n",
    "    \n",
    "## 0 - Environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "import pickle\n",
    "\n",
    "# \"Tableau 20\" colors as RGB for plotting\n",
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]  \n",
    "  \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts\n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load Models and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_classifier = pickle.load(open('./models/ridge_full-cohort', 'rb'))\n",
    "ridge_classifier_HR = pickle.load(open('./models/ridge_HR-cohort', 'rb'))\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.load_model('./models/xgb_full-cohort')\n",
    "xgb_classifier_HR = XGBClassifier()\n",
    "xgb_classifier_HR.load_model('./models/xgb_HR-cohort')\n",
    "\n",
    "test_X = pd.read_csv('../extraction/data/test_X.csv').set_index('patientunitstayid').values\n",
    "test_y = pd.read_csv('../extraction/data/test_y.csv').values.ravel()\n",
    "test_apache = pd.read_csv('../extraction/data/test_apache.csv').values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Model Evaluation\n",
    "\n",
    "We'll now evaluate these models on discriminatory capability and calibration. As we are only interested in performance in the high-risk subset, we'll examine only the high-risk patients in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_HR = test_X[(test_apache > 0.10), :]\n",
    "test_y_HR = test_y[test_apache > 0.10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want a helper function for calculating 95% confidence intervals for all of the metrics we generate. These will be attained conservatively via bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_ci(f_hat, y_true, n_bootstraps=2000, ci_level=0.95):\n",
    "    li = (1. - ci_level)/2\n",
    "    ui = 1 - li\n",
    "\n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    bootstrapped_auc = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(f_hat), len(f_hat))\n",
    "        auc = roc_auc_score(y_true[indices], f_hat[indices])\n",
    "        bootstrapped_auc.append(auc)\n",
    "\n",
    "    sorted_scores = np.array(bootstrapped_auc)\n",
    "    sorted_scores.sort()\n",
    "    confidence_lower = sorted_scores[int(li * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(ui * len(sorted_scores))]\n",
    "\n",
    "    return confidence_lower, confidence_upper\n",
    "\n",
    "def brier_ci(f_hat, y_true, n_bootstraps=2000, ci_level=0.95):\n",
    "    li = (1. - ci_level)/2\n",
    "    ui = 1. - li\n",
    "\n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    bootstrapped_bs = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(f_hat), len(f_hat))\n",
    "        bs = brier_score_loss(y_true[indices], f_hat[indices])\n",
    "        bootstrapped_bs.append(bs)\n",
    "\n",
    "    sorted_scores = np.array(bootstrapped_bs)\n",
    "    sorted_scores.sort()\n",
    "    confidence_lower = sorted_scores[int(li * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(ui * len(sorted_scores))]\n",
    "\n",
    "    return confidence_lower, confidence_upper\n",
    "\n",
    "def op_ratio_ci(f_hat, y_true, n_bootstraps=2000, ci_level=0.95):\n",
    "    li = (1. - ci_level)/2\n",
    "    ui = 1. - li\n",
    "\n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    bootstrapped_opr = []\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(f_hat), len(f_hat))\n",
    "        opr = y_true[indices].mean() / f_hat[indices].mean()\n",
    "        bootstrapped_opr.append(opr)\n",
    "\n",
    "    sorted_scores = np.array(bootstrapped_opr)\n",
    "    sorted_scores.sort()\n",
    "    confidence_lower = sorted_scores[int(li * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(ui * len(sorted_scores))]\n",
    "\n",
    "    return confidence_lower, confidence_upper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discrimination, Full Cohort Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_hat_apache = test_apache[test_apache > 0.10]\n",
    "roc_apache = roc_curve(test_y_HR, f_hat_apache)\n",
    "auc_apache = roc_auc_score(test_y_HR, f_hat_apache)\n",
    "\n",
    "f_hat_ridge = ridge_classifier.predict_proba(test_X_HR)\n",
    "roc_ridge = roc_curve(test_y_HR, f_hat_ridge[:, 1])\n",
    "auc_ridge = roc_auc_score(test_y_HR, f_hat_ridge[:, 1])\n",
    "\n",
    "f_hat_xgb = xgb_classifier.predict_proba(test_X_HR)\n",
    "roc_xgb = roc_curve(test_y_HR, f_hat_xgb[:, 1])\n",
    "auc_xgb = roc_auc_score(test_y_HR, f_hat_xgb[:, 1])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(roc_apache[0], roc_apache[1], color=tableau20[12], \n",
    "         label='APACHE IV\\n(AUC = {0:.3f} [{1:.3f}, {2:.3f}])'.format(auc_apache, *auc_ci(f_hat_apache, test_y_HR)))\n",
    "plt.plot(roc_ridge[0], roc_ridge[1], color=tableau20[16], \n",
    "         label='Ridge\\n(AUC = {0:.3f} [{1:.3f}, {2:.3f}])'.format(auc_ridge, *auc_ci(f_hat_ridge[:, 1], test_y_HR)))\n",
    "plt.plot(roc_xgb[0], roc_xgb[1], color=tableau20[7], \n",
    "         label='XGB\\n(AUC = {0:.3f} [{1:.3f}, {2:.3f}])'.format(auc_xgb, *auc_ci(f_hat_xgb[:, 1], test_y_HR)))\n",
    "plt.plot([0, 1], [0, 1], 'k:', color=tableau20[0])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC: Full Cohort Models')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calibration, Full Cohort Models__\n",
    "\n",
    "We first calculate observed-to-predicted ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full Cohort Models: Observed-to-predicted Ratios')\n",
    "print('Overall Subcohort Mortality: {0:.3f}'.format(test_y_HR.mean()))\n",
    "print('APACHE IVa OPR: {0:.3f} [{1:.3f}, {2:.3f}]'.format(test_y_HR.mean()/f_hat_apache.mean(), *op_ratio_ci(f_hat_apache, test_y_HR)))\n",
    "print('Ridge OPR: {0:.3f} [{1:.3f}, {2:.3f}]'.format(test_y_HR.mean()/f_hat_ridge[:, 1].mean(), *op_ratio_ci(f_hat_ridge[:, 1], test_y_HR)))\n",
    "print('XGB OPR: {0:.3f} [{1:.3f}, {2:.3f}]'.format(test_y_HR.mean()/f_hat_xgb[:, 1].mean(), *op_ratio_ci(f_hat_xgb[:, 1], test_y_HR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate reliability curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_of_positives_apache, mean_predicted_value_apache = calibration_curve(test_y_HR, f_hat_apache, n_bins=10)\n",
    "fraction_of_positives_ridge, mean_predicted_value_ridge = calibration_curve(test_y_HR, f_hat_ridge[:, 1], n_bins=10)\n",
    "fraction_of_positives_xgb, mean_predicted_value_xgb = calibration_curve(test_y_HR, f_hat_xgb[:, 1], n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax1 = plt.axes()\n",
    "ax1.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated', color=tableau20[0])\n",
    "\n",
    "ax1.plot(mean_predicted_value_apache, fraction_of_positives_apache, color=tableau20[12], \n",
    "         label='APACHE IV')\n",
    "ax1.plot(mean_predicted_value_ridge, fraction_of_positives_ridge, color=tableau20[16], \n",
    "         label='Ridge')\n",
    "ax1.plot(mean_predicted_value_xgb, fraction_of_positives_xgb, color=tableau20[7], \n",
    "         label='XGB')\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration (reliability curve): Full Cohort Models')\n",
    "ax1.set_xlabel(\"Mean predicted value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Brier Scores, Full Cohort Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_apache = brier_score_loss(test_y_HR, f_hat_apache)\n",
    "bs_ridge = brier_score_loss(test_y_HR, f_hat_ridge[:, 1])\n",
    "bs_xgb = brier_score_loss(test_y_HR, f_hat_xgb[:, 1])\n",
    "\n",
    "print('Brier Scores for Full Cohort Models')\n",
    "print('APACHE IV Brier Score: {0:.3f} [{1:.3f}, {2:.3f}]'.format(bs_apache, *brier_ci(f_hat_apache, test_y_HR)))\n",
    "print('Ridge Brier Score: {0:.3f} [{1:.3f}, {2:.3f}]'.format(bs_ridge, *brier_ci(f_hat_ridge[:, 1], test_y_HR)))\n",
    "print('XGB Brier Score: {0:.3f} [{1:.3f}, {2:.3f}]'.format(bs_xgb, *brier_ci(f_hat_xgb[:, 1], test_y_HR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discrimination, High-risk Subset Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_hat_ridge_HR = ridge_classifier_HR.predict_proba(test_X_HR)\n",
    "roc_ridge_HR = roc_curve(test_y_HR, f_hat_ridge_HR[:, 1])\n",
    "auc_ridge_HR = roc_auc_score(test_y_HR, f_hat_ridge_HR[:, 1])\n",
    "\n",
    "f_hat_xgb_HR = xgb_classifier_HR.predict_proba(test_X_HR)\n",
    "roc_xgb_HR = roc_curve(test_y_HR, f_hat_xgb_HR[:, 1])\n",
    "auc_xgb_HR = roc_auc_score(test_y_HR, f_hat_xgb_HR[:, 1])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(roc_apache[0], roc_apache[1], color=tableau20[12], \n",
    "         label='APACHE IV\\n(AUC = {0:.3f} [{1:.3f}, {2:.3f}])'.format(auc_apache, *auc_ci(f_hat_apache, test_y_HR)))\n",
    "plt.plot(roc_ridge_HR[0], roc_ridge_HR[1], color=tableau20[16], \n",
    "         label='Ridge\\n(AUC = {0:.3f} [{1:.3f}, {2:.3f}])'.format(auc_ridge_HR, *auc_ci(f_hat_ridge_HR[:, 1], test_y_HR)))\n",
    "plt.plot(roc_xgb_HR[0], roc_xgb_HR[1], color=tableau20[7], \n",
    "         label='XGB\\n(AUC = {0:.3f} [{1:.3f}, {2:.3f}])'.format(auc_xgb_HR, *auc_ci(f_hat_xgb_HR[:, 1], test_y_HR)))\n",
    "plt.plot([0, 1], [0, 1], 'k:', color=tableau20[0])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC: High-risk Sub-cohort Models')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calibration, High-risk Subset Models__\n",
    "\n",
    "Again, we first produce observed-to-predicted ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High-risk Sub-cohort Models: Observed-to-predicted Ratios')\n",
    "print('Overall Subcohort Mortality: {0:.3f}'.format(test_y_HR.mean()))\n",
    "print('APACHE IVa OPR: {0:.3f} [{1:.3f}, {2:.3f}]'.format(test_y_HR.mean()/f_hat_apache.mean(), *op_ratio_ci(f_hat_apache, test_y_HR)))\n",
    "print('Ridge OPR: {0:.3f} [{1:.3f}, {2:.3f}]'.format(test_y_HR.mean()/f_hat_ridge_HR[:, 1].mean(), *op_ratio_ci(f_hat_ridge_HR[:, 1], test_y_HR)))\n",
    "print('XGB OPR: {0:.3f} [{1:.3f}, {2:.3f}]'.format(test_y_HR.mean()/f_hat_xgb_HR[:, 1].mean(), *op_ratio_ci(f_hat_xgb_HR[:, 1], test_y_HR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then reliability curves for our high-risk models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_of_positives_ridge_HR, mean_predicted_value_ridge_HR = calibration_curve(test_y_HR, f_hat_ridge_HR[:, 1], n_bins=10)\n",
    "fraction_of_positives_xgb_HR, mean_predicted_value_xgb_HR = calibration_curve(test_y_HR, f_hat_xgb_HR[:, 1], n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax1 = plt.axes()\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly Calibrated\", color=tableau20[0])\n",
    "\n",
    "ax1.plot(mean_predicted_value_apache, fraction_of_positives_apache, color=tableau20[12], \n",
    "         label='APACHE IV')\n",
    "ax1.plot(mean_predicted_value_ridge_HR, fraction_of_positives_ridge_HR, color=tableau20[16], \n",
    "         label='Ridge')\n",
    "ax1.plot(mean_predicted_value_xgb_HR, fraction_of_positives_xgb_HR, color=tableau20[7], \n",
    "         label='XGB')\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration (reliability curve): High-risk Sub-cohort Models')\n",
    "ax1.set_xlabel(\"Mean predicted value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Brier Scores, High-risk Subset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_ridge_HR = brier_score_loss(test_y_HR, f_hat_ridge_HR[:, 1])\n",
    "bs_xgb_HR = brier_score_loss(test_y_HR, f_hat_xgb_HR[:, 1])\n",
    "\n",
    "print('Brier Scores for High-risk Subcohort Models')\n",
    "print('APACHE IV Brier Score: {0:.3f} [{1:.3f}, {2:.3f}]'.format(bs_apache, *brier_ci(f_hat_apache, test_y_HR)))\n",
    "print('Ridge Brier Score: {0:.3f} [{1:.3f}, {2:.3f}]'.format(bs_ridge_HR, *brier_ci(f_hat_ridge_HR[:, 1], test_y_HR)))\n",
    "print('XGB Brier Score: {0:.3f} [{1:.3f}, {2:.3f}]'.format(bs_xgb_HR, *brier_ci(f_hat_xgb_HR[:, 1], test_y_HR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Feature Analysis\n",
    "\n",
    "The features of the ridge models may be examined via the odds ratios associated with each feature. Similary, although not directly comparable, the importance matrix of the gradient boosted models may provide insight.\n",
    "\n",
    "We first extract the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pd.read_csv('../extraction/data/train_X.csv').set_index('patientunitstayid').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ridge models, we'll manually extract the coefficient array, which has its order preserved, and assign the feature names so that we can easily read the sorted list. We examine the full cohort model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coef = {k:np.exp(v) for k, v in zip(feature_names, ridge_classifier.named_steps['ridge'].coef_.ravel())}\n",
    "ridge_coef = pd.DataFrame.from_dict(ridge_coef, orient='index', columns=['odds_ratio'])\n",
    "ridge_coef.reindex(ridge_coef.odds_ratio.abs().sort_values(ascending=False).index).iloc[0:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we turn to the subcohort model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coef_HR = {k:np.exp(v) for k, v in zip(feature_names, ridge_classifier_HR.named_steps['ridge'].coef_.ravel())}\n",
    "ridge_coef_HR = pd.DataFrame.from_dict(ridge_coef_HR, orient='index', columns=['odds_ratio'])\n",
    "ridge_coef_HR.reindex(ridge_coef_HR.odds_ratio.abs().sort_values(ascending=False).index).iloc[0:20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 20 features in both by OR magnitude are similar for both the full and subcohort models, and, intuitevly we note certain features in the high-risk model that we would expect to see with more prevalence in a sicker cohort e.g. metastasis (`mets`).\n",
    "\n",
    "We next turn to the XGB models. For these we'll need to map the feature names onto the models internal representation of the features and thene xtract the importance matrices. Again we start with the full cohort models. XGB also provides a nice function for visualizing the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {'f{0}'.format(i): v for i, v in enumerate(feature_names)}\n",
    "imp_matrix = {mapper[k]: round(v, 3) for k, v in xgb_classifier.get_booster().get_score(importance_type='gain').items()}\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_importance(imp_matrix, max_num_features=20, color=tableau20[6], importance_type='gain', xlabel='Information Gain', ax=plt.axes())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same mapper, we can generate the same result for the high-risk subcohort XGB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_matrix = {mapper[k]: round(v, 3) for k, v in xgb_classifier_HR.get_booster().get_score(importance_type='gain').items()}\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_importance(imp_matrix, max_num_features=20, color=tableau20[6], importance_type='gain', xlabel='Information Gain', ax=plt.axes())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note similarity between these features and the ridge features and substantial overlap between the XGB full and high-risk models. Overall we have a pretty good intuition as to why these are the biggest contributions, and differences between full and high-risk subcohort models are not marked."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
