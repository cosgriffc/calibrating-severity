{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity Score Calibration\n",
    "## Cohort Extraction\n",
    "### C.V. Cosgriff, MIT Critical Data\n",
    "\n",
    "Here we will extract the necessary variables for feature engineering and cohort construction to build a number of simple mortality models. These models will be trained in the entire cohort and in risk-stratified subcohorts in an attempt to examine if model calibration can be improved by simply training on populations with improved case-severity homogeneity.\n",
    "\n",
    "The features we will extract are as follows:\n",
    "\n",
    "* Age, gender, weight (`patient` table)\n",
    "* Source of admission, unit type (`patient` table)\n",
    "* Laboratory data  (`labsfirstday` materialized view)\n",
    "    * Blood gases: PaO2, pH, base excess, bicarbonate,\n",
    "    * Hematology: hematocrit, hemoglobin, lymphocytes, neutrophils, platelets, white cell count\n",
    "    * Electrolytes: calcium, chloride, ionized calcium, magnesium, phosphate, sodium\n",
    "    * Biochemistry: albumin, amylase, bilirubin, blood urea nitrogen (BUN), B-natriuretic peptide, creatine phosphokinase (cpk), creatinine, lactate, lipase, troponin I/T, pH, bicarbonate, base excess, glucose\n",
    "    * Coagulation: PT/INR, fibrinogen\n",
    "* Vital signs (`vitalsfirstday` materialized view)\n",
    "* Diagnoses: Modified Elixhauser (`diagnosis_groups.R`)\n",
    "* Treatments (`treatmentfirstday` materializd view)\n",
    "    * Antiarrhythmics\n",
    "    * Antibiotics\n",
    "    * Vasopressors\n",
    "    * Sedatives\n",
    "    * Diuretics\n",
    "    * Blood products\n",
    "* Ventilation status (`apachepredvar` table)\n",
    "* APACHE IV Features (`apachepredvar` table)\n",
    "\n",
    "The label for our classifier as well as their baseline APACHE IV predicted mortality are located in `apachepatientresult`.\n",
    "\n",
    "## 0 - Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# postgres envrionment setup; placeholds here, place your own info\n",
    "dbname = 'eicu'\n",
    "schema_name = 'eicu_crd'\n",
    "query_schema = 'SET search_path TO ' + schema_name + ';'\n",
    "\n",
    "# connect to the database\n",
    "con = psycopg2.connect(dbname = dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Materialized Views\n",
    "\n",
    "We will generate the requisite materialized views to aid the extraction of the cohort features. We start by introducing helper functions for interacting with the eICU database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query_safely(sql, con):\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "    except:\n",
    "        # if an exception, rollback, rethrow the exception - finally closes the connection\n",
    "        cur.execute('rollback;')\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "    return\n",
    "\n",
    "def generate_materialized_view(query_file, query_schema):\n",
    "    with open(query_file) as fp:\n",
    "        query = ''.join(fp.readlines())\n",
    "    print('Generating materialized view using {} ...'.format(query_file), end = ' ')\n",
    "    execute_query_safely(query_schema + query, con)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating materialized view using ./sql/vitalsfirstday.sql ... done.\n",
      "Generating materialized view using ./sql/labsfirstday.sql ... done.\n",
      "Generating materialized view using ./sql/treatmentfirstday.sql ... done.\n"
     ]
    }
   ],
   "source": [
    "generate_materialized_view('./sql/vitalsfirstday.sql', query_schema)\n",
    "generate_materialized_view('./sql/labsfirstday.sql', query_schema)\n",
    "generate_materialized_view('./sql/treatmentfirstday.sql', query_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an R script from a previous project to assign diagnosis groupings. These groupings were based off the ICD-9 codes from the Elixhauser system, but were modified and do not represent valid Elixhuaser scoring groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required package: methods\n",
      "Loading required package: DBI\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Attaching package: ‘dbplyr’\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    ident, sql\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!Rscript ./R/diagnosis_groups.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading a base cohort. As our most import exclusion criteria is whether or not they had an APACHE IVa score (so we can fit models in the subpopulations), we can join the `patient` table on the `apachepredvar` and `apachepatientresult` table. There are 200,859 ICU stays, and so the number of rows returned will be the remainder aftr exclusion of patients for which APACHE data is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146689, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cohort_query = query_schema + '''\n",
    "WITH apacheIV AS\n",
    "(\n",
    "    SELECT patientunitstayid, apachescore\n",
    "         , predictedhospitalmortality, actualhospitalmortality \n",
    "    FROM apachepatientresult\n",
    "    WHERE apacheversion = 'IV'\n",
    "    AND apachescore > 0\n",
    ")\n",
    "\n",
    "SELECT p.patientunitstayid, p.age, p.gender, p.ethnicity, p.admissionheight AS height\n",
    "       , p.admissionweight AS weight , p.unittype AS unit_type, p.unitadmitsource\n",
    "       , p.unitdischargeoffset AS unit_los, p.hospitaldischargeoffset AS hospital_los\n",
    "       , a.day1meds AS gcs_meds, a.day1verbal AS gcs_verbal, a.day1motor AS gcs_motor\n",
    "       , a.day1eyes AS gcs_eyes, a.admitDiagnosis AS admit_diagnosis\n",
    "       , a.thrombolytics AS apache_thrombolytics, a.electivesurgery AS apache_elect_surg\n",
    "       , activetx AS apache_active_tx, a.readmit AS apache_readmit, a.ima AS apache_ima\n",
    "       , a.midur AS apache_midur, a.ventday1 AS apache_ventday1_worst\n",
    "       , a.oOBVentDay1 AS apache_ventday1, a.oOBIntubDay1 AS apache_intubday1\n",
    "       , a.day1fio2 AS apache_fio2, a.day1pao2 AS apache_pao2, (a.day1pao2 / a.day1fio2) AS apache_o2ratio\n",
    "       , a.ejectfx AS apache_ejectfx, a.creatinine AS apache_creatinine\n",
    "       , a.graftcount AS apache_graftcount, o.predictedhospitalmortality AS apache_prediction\n",
    "       , o.apachescore AS apache_score, o.actualhospitalmortality AS hospital_expiration\n",
    "FROM patient p\n",
    "INNER JOIN apachepredvar a\n",
    "ON p.patientunitstayid = a.patientunitstayid\n",
    "INNER JOIN apacheIV o\n",
    "ON p.patientunitstayid = o.patientunitstayid\n",
    "ORDER BY patientunitstayid;\n",
    "'''\n",
    "base_cohort = pd.read_sql_query(base_cohort_query, con)\n",
    "base_cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 200,859 patients in the database, 146,689 have APACHE IV variables recorded as well as a prediction carried out. We'll then load the rest of the feature set into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145578, 85)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_query = query_schema + '''\n",
    "SELECT v.patientunitstayid, v.HR_Mean, v.SBP_periodic_Mean, v.DBP_periodic_Mean\n",
    "    , v.MAP_periodic_Mean, v.SBP_aperiodic_Mean, v.DBP_aperiodic_Mean\n",
    "    , v.MAP_aperiodic_Mean, v.RR_Mean, v.SpO2_Mean, v.TempC_Mean\n",
    "    , ANIONGAP_min, ANIONGAP_max, ALBUMIN_min, ALBUMIN_max \n",
    "    , AMYLASE_min, AMYLASE_max, BASEEXCESS_min, BASEEXCESS_max\n",
    "    , BICARBONATE_min, BICARBONATE_max, BUN_min, BUN_max, BNP_min\n",
    "    , BNP_max, CPK_min, CPK_max, BILIRUBIN_min, BILIRUBIN_max\n",
    "    , CALCIUM_min, CALCIUM_max, IONCALCIUM_min, IONCALCIUM_max\n",
    "    , CREATININE_min, CREATININE_max, CHLORIDE_min, CHLORIDE_max\n",
    "    , GLUCOSE_min, GLUCOSE_max, HEMATOCRIT_min, HEMATOCRIT_max\n",
    "    , FIBRINOGEN_min, FIBRINOGEN_max, LIPASE_min, LIPASE_max\n",
    "    , HEMOGLOBIN_min, HEMOGLOBIN_max, LACTATE_min, LACTATE_max\n",
    "    , LYMPHS_min, LYMPHS_max, MAGNESIUM_min, MAGNESIUM_max\n",
    "    , PAO2_min, PAO2_max, PH_min, PH_max, PLATELET_min\n",
    "    , PLATELET_max, PMN_min, PMN_max, PHOSPHATE_min, PHOSPHATE_max\n",
    "    , POTASSIUM_min, POTASSIUM_max, PTT_min, PTT_max, INR_min\n",
    "    , INR_max, PT_min, PT_max, SODIUM_min, SODIUM_max\n",
    "    , TROPI_min, TROPI_max, TROPT_min, TROPT_max, WBC_min\n",
    "    , WBC_max, t.abx, t.pressor, t.antiarr, t.sedative\n",
    "    , t.diuretic, t.blood_product\n",
    "FROM vitalsfirstday v\n",
    "INNER JOIN labsfirstday l\n",
    "ON v.patientunitstayid = l.patientunitstayid\n",
    "INNER JOIN treatmentfirstday t\n",
    "ON v.patientunitstayid = t.patientunitstayid;\n",
    "'''\n",
    "\n",
    "feature_set = pd.read_sql_query(feature_set_query, con)\n",
    "feature_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then merge the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119884, 117)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = pd.merge(base_cohort, feature_set, on='patientunitstayid')\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we lose ~30,000 patients who did not have data in one of the feature tables. Finally, we load the diagnosis groupings CSV to assign this last set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119776, 147)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx_groups = pd.read_csv('./dx-firstday_groupings.csv')\n",
    "cohort = pd.merge(cohort, dx_groups, on='patientunitstayid')\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Inclusion / Exclusion\n",
    "\n",
    "By nature of our SQL query, we have already excluded patients not eligible/capable of producing a valid score, and patients who lack all vital/lab/treatment data. We can then apply the following and keep only patients who are:\n",
    "\n",
    "1. Not readmissions\n",
    "2. Not admitted from another ICU\n",
    "3. Admitted to ICU for $\\geq4$hours\n",
    "4. Hospital admission $\\lt365$days\n",
    "5. Not burn patients\n",
    "6. Age $\\geq16$, age $\\lt89$, and age not missing\n",
    "\n",
    "__1 - Readmitted__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113179, 147)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.apache_readmit == 0, :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - Not Admitted from ICU__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112696, 147)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.unitadmitsource != 'Other ICU', :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - ICU LoS >4h__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112696, 147)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.unit_los >= 240, :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Hospital LoS $\\lt$365__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112684, 147)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.hospital_los < 525600, :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - Not Burn Patients__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112673, 147)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.admit_diagnosis != 'BURN', :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - Age $\\geq$16, $\\leq$89, and Not Missing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108547, 147)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = cohort.loc[cohort.age != '> 89', :]\n",
    "cohort = cohort.loc[cohort.age != '', :]\n",
    "cohort.age = cohort.age.astype('int64')\n",
    "cohort = cohort.loc[cohort.age >= 16, :]\n",
    "cohort.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Cleaning and Formatting\n",
    "\n",
    "Before anything else, we can drop numerous variables from the pull as we won't need both min/max for various features. Instead we'll be going by the following principle: the most _abnormal_ laboratory value in the first 24 hours of ICU stay will be included. That is we will use:\n",
    "\n",
    "* The minimum value for: bicarbonate, chloride, calcium, magnesium, base excess (including negative values), platelets, hemoglobin, phosphate, fibrinogen, pH and hematocrit\n",
    "* The maximum value for: creatinine, BUN, bilirubin, PT/INR, lactate, troponin I/T, amylase, lipase, B-natriuretic peptide and creatinine phosphokinase\n",
    "* For sodium, which aberrantly deviates bidirectionally, the most abnormal value was defined as the value with greatest deviation from the normal range boundaries.\n",
    "    * This can be applied to glucose and potassium as well.\n",
    "* For white blood cell and neutrophil counts, if any measurements were lower than the lower limit of normal, the minimum value was used; if the minimum was within normal range then the maximum was used as the most abnormal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the unidirectional abberations, just drop what isn't needed\n",
    "lab_drop = ['bicarbonate_max', 'chloride_max', 'calcium_max', 'magnesium_max', \n",
    "            'baseexcess_max', 'platelet_max', 'hemoglobin_max', 'phosphate_max', \n",
    "            'fibrinogen_max', 'ph_max', 'hematocrit_max', 'creatinine_min',\n",
    "            'bun_min', 'bilirubin_min', 'pt_min', 'inr_min', 'lactate_min', \n",
    "            'tropi_min', 'tropt_min', 'amylase_min', 'lipase_min', 'bnp_min',\n",
    "            'cpk_min', 'albumin_max', 'ioncalcium_max', 'pao2_max', 'pt_min',\n",
    "            'ptt_min', 'inr_min', 'aniongap_min']\n",
    "cohort = cohort.drop(lab_drop, axis=1)\n",
    "\n",
    "# sodium, deviates bidirectionally\n",
    "sodium_check = abs(cohort.sodium_min - 135.) >= abs(cohort.sodium_max - 145.)\n",
    "sodium = np.empty(len(cohort.index), dtype='float64')\n",
    "sodium[sodium_check] = cohort.sodium_min[sodium_check]\n",
    "sodium[~sodium_check] = cohort.sodium_max[~sodium_check]\n",
    "cohort = cohort.assign(sodium=sodium)\n",
    "cohort = cohort.drop(['sodium_min', 'sodium_max'], axis=1)\n",
    "\n",
    "# potassium, deviates bidirectionally, same treatment then\n",
    "potassium_check = abs(cohort.potassium_min - 3.5) >= abs(cohort.potassium_max - 5.0)\n",
    "potassium = np.empty(len(cohort.index), dtype='float64')\n",
    "potassium[potassium_check] = cohort.potassium_min[potassium_check]\n",
    "potassium[~potassium_check] = cohort.potassium_max[~potassium_check]\n",
    "cohort = cohort.assign(potassium=potassium)\n",
    "cohort = cohort.drop(['potassium_min', 'potassium_max'], axis=1)\n",
    "\n",
    "# similar treatment for glucose since hyperglycemia and hypoglycemia can both\n",
    "# be important dependent on the clinical context.\n",
    "glucose_check = abs(cohort.glucose_min - 70) >= abs(cohort.glucose_max - 130)\n",
    "glucose = np.empty(len(cohort.index), dtype='float64')\n",
    "glucose[glucose_check] = cohort.glucose_min[glucose_check]\n",
    "glucose[~glucose_check] = cohort.glucose_max[~glucose_check]\n",
    "cohort = cohort.assign(glucose=glucose)\n",
    "cohort = cohort.drop(['glucose_min', 'glucose_max'], axis=1)\n",
    "\n",
    "# wbc counts\n",
    "wbc_check = cohort.wbc_min < 2\n",
    "pmn_check = cohort.pmn_min < 45\n",
    "lym_check = cohort.lymphs_min < 20\n",
    "\n",
    "wbc = np.empty(len(cohort.index), dtype='float64')\n",
    "wbc[wbc_check] = cohort.wbc_min[wbc_check]\n",
    "wbc[~wbc_check] = cohort.wbc_max[~wbc_check]\n",
    "cohort = cohort.assign(wbc=wbc)\n",
    "cohort = cohort.drop(['wbc_min', 'wbc_max'], axis=1)\n",
    "\n",
    "pmn = np.empty(len(cohort.index), dtype='float64')\n",
    "pmn[pmn_check] = cohort.pmn_min[pmn_check]\n",
    "pmn[~pmn_check] = cohort.pmn_max[~pmn_check]\n",
    "cohort = cohort.assign(pmn=pmn)\n",
    "cohort = cohort.drop(['pmn_min', 'pmn_max'], axis=1)\n",
    "\n",
    "lym = np.empty(len(cohort.index), dtype='float64')\n",
    "lym[lym_check] = cohort.lymphs_min[lym_check]\n",
    "lym[~lym_check] = cohort.lymphs_max[~lym_check]\n",
    "cohort = cohort.assign(lym=lym)\n",
    "cohort = cohort.drop(['lymphs_min', 'lymphs_max'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect there to be substantial missing data in the EHR. We also note on inspection that some of the missing data in eICU is simply coded as a -1, and this is evident even in the APACHE predictions which are required for inclusion. As such, we'll swap -1 with `np.nan`, and then remove patients missing their APACHE prediction. From their we'll inspect the missing data quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patientunitstayid             0\n",
       "age                           0\n",
       "gender                        0\n",
       "ethnicity                     0\n",
       "height                     1493\n",
       "weight                     1280\n",
       "unit_type                     0\n",
       "unitadmitsource               0\n",
       "unit_los                      0\n",
       "hospital_los                  0\n",
       "gcs_meds                      0\n",
       "gcs_verbal                 1178\n",
       "gcs_motor                  1178\n",
       "gcs_eyes                   1178\n",
       "admit_diagnosis               0\n",
       "apache_thrombolytics          0\n",
       "apache_elect_surg         86437\n",
       "apache_active_tx              0\n",
       "apache_readmit                0\n",
       "apache_ima                    0\n",
       "apache_midur                  0\n",
       "apache_ventday1_worst         0\n",
       "apache_ventday1               0\n",
       "apache_intubday1              0\n",
       "apache_fio2               82300\n",
       "apache_pao2               82300\n",
       "apache_o2ratio                0\n",
       "apache_ejectfx           105424\n",
       "apache_creatinine         20486\n",
       "apache_graftcount             0\n",
       "apache_prediction             0\n",
       "apache_score                  0\n",
       "hospital_expiration           0\n",
       "hr_mean                      16\n",
       "sbp_periodic_mean         81514\n",
       "dbp_periodic_mean         81529\n",
       "map_periodic_mean         81572\n",
       "sbp_aperiodic_mean          352\n",
       "dbp_aperiodic_mean          378\n",
       "map_aperiodic_mean          992\n",
       "rr_mean                    7128\n",
       "spo2_mean                  1131\n",
       "tempc_mean                96865\n",
       "aniongap_max              28230\n",
       "albumin_min               30877\n",
       "amylase_max              101771\n",
       "baseexcess_min            86994\n",
       "bicarbonate_min            9541\n",
       "bun_max                    3069\n",
       "bnp_max                   91501\n",
       "cpk_max                   78254\n",
       "bilirubin_max             35311\n",
       "calcium_min                6385\n",
       "ioncalcium_min            85039\n",
       "creatinine_max             2903\n",
       "chloride_min               3557\n",
       "hematocrit_min             3454\n",
       "fibrinogen_min            98684\n",
       "lipase_max                90902\n",
       "hemoglobin_min             4099\n",
       "lactate_max               65450\n",
       "magnesium_min             40308\n",
       "pao2_min                  58156\n",
       "ph_min                    59290\n",
       "platelet_min               5386\n",
       "phosphate_min             64640\n",
       "ptt_max                   55528\n",
       "inr_max                   38128\n",
       "pt_max                    40482\n",
       "tropi_max                 76837\n",
       "tropt_max                101332\n",
       "abx                           0\n",
       "pressor                       0\n",
       "antiarr                       0\n",
       "sedative                      0\n",
       "diuretic                      0\n",
       "blood_product                 0\n",
       "Unnamed: 0                    0\n",
       "chf                           0\n",
       "arryhthmia                    0\n",
       "valvdz                        0\n",
       "pulmcirc                      0\n",
       "pvd                           0\n",
       "dm                            0\n",
       "renalfail                     0\n",
       "liverdz                       0\n",
       "hypothyroid                   0\n",
       "pud                           0\n",
       "neuro                         0\n",
       "pulm                          0\n",
       "htn                           0\n",
       "paralysis                     0\n",
       "hiv                           0\n",
       "lymphoma                      0\n",
       "mets                          0\n",
       "solidca                       0\n",
       "cvd                           0\n",
       "coagdef                       0\n",
       "obesity                       0\n",
       "wtloss                        0\n",
       "electrolyte                   0\n",
       "blanemia                      0\n",
       "defanemia                     0\n",
       "drugabuse                     0\n",
       "etohabuse                     0\n",
       "psychoses                     0\n",
       "depression                    0\n",
       "sodium                     2943\n",
       "potassium                  3285\n",
       "glucose                    3490\n",
       "wbc                        3972\n",
       "pmn                       37857\n",
       "lym                       28920\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cohort = cohort.replace(-1, np.nan)\n",
    "cohort.apache_prediction = cohort.apache_prediction.astype('float64')\n",
    "cohort = cohort.loc[cohort.apache_prediction >= 0.,]\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(cohort.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 1178 patients missing GCS terms, but this actually agrees with the GCS meds indicator.\n",
    "    * Thus this serves as an indicator of why those are missing\n",
    "* For unclear reasons, the elective surgery indiactor is missing for many patients in the APACHE table. I had presumed this was just left blank and this `nan` = `0` but there are also 0s in the data. With the admit diagnosis and other data it is unlikely to be a very important variable and we'll drop it.\n",
    "* The temperature variable come from the periodic sensor table, which can be a rather dirty source of data. Considering how much data is missing for this variable and the data source, we'll drop it.\n",
    "* We have PaO2 data from both the APACHE table and from our direct pull of the labs; yet there is much more missing data in the APACHE variable version. We'll drop it and just use the direct pull. This is similarly true for creatinine. \n",
    "* We'll use aperiodic recordings of the BP since the periodic recording is missing in so many patients\n",
    "* An index `Unnamed 0` was picked up from the diagnoses table, we'll drop it\n",
    "* Many of the labs are missing for many patients; as was shown recently from Kohane's group the presence of a lab is often more informative than its value. That said, GBM as implemented in xgboost can actually learn from missingness and is often fairly robust to large amounts of missing data.\n",
    "    * We could therefore just leave the missing data as is\n",
    "    * We could perform mean imputation\n",
    "    * We could impute _normal_ values\n",
    "    * We could implement a more sophisticated imputation approach\n",
    "* We don't want LoS variables in there since they let us _peak into the future_.\n",
    "* We only needed unitadmitsource for the exclusion criteria and can drop it.\n",
    "* We'll save off the label (`hospital_expiration`) and APACHE IV predicted probability later, but can drop the APACHE IV score now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = cohort.drop(['sbp_periodic_mean', 'dbp_periodic_mean', 'map_periodic_mean',\n",
    "                      'Unnamed: 0', 'tempc_mean', 'apache_elect_surg', 'apache_creatinine', \n",
    "                      'apache_pao2', 'apache_fio2', 'apache_o2ratio', 'hospital_los', 'unit_los', \n",
    "                      'unitadmitsource', 'apache_score',], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these gone, we can decide on the remainder of missingness handling during modeling. We next turn to formatting the data so that it will be amenable to modeling. This entails converting categorical variables into indicators, and thus we must first convert the strings composing the categories into good variable names.\n",
    "\n",
    "We'll start with admission diagnoses which include '-' characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.admit_diagnosis = cohort.admit_diagnosis.str.replace('-', '_')\n",
    "cohort.admit_diagnosis = cohort.admit_diagnosis.str.replace('/', '_')\n",
    "adx_dummies = pd.get_dummies(cohort.admit_diagnosis, 'adx', drop_first=True)\n",
    "cohort = pd.concat([cohort, adx_dummies], axis=1)\n",
    "cohort = cohort.drop('admit_diagnosis', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we turn to gender and ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_gender = (cohort.gender == 'Male').astype('int')\n",
    "cohort = cohort.assign(male_gender=male_gender)\n",
    "cohort = cohort.drop('gender', axis=1)\n",
    "\n",
    "eth_map = {'Caucasian' : 'caucasian', 'Other/Unknown' : 'other', \n",
    "           'Native American' : 'native_american', 'African American' : 'african_american',\n",
    "          'Asian' : 'asian', 'Hispanic' : 'hispanic', '' : 'other'}\n",
    "cohort.ethnicity = cohort.ethnicity.map(eth_map)\n",
    "eth_dummies = pd.get_dummies(cohort.ethnicity, 'eth', drop_first=True)\n",
    "cohort = pd.concat([cohort, eth_dummies], axis=1)\n",
    "cohort = cohort.drop('ethnicity', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves unit type as a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.unit_type = cohort.unit_type.str.replace('-', '_')\n",
    "cohort.unit_type = cohort.unit_type.str.replace(' ', '_')\n",
    "unit_dummies = pd.get_dummies(cohort.unit_type, 'unit', drop_first=True)\n",
    "cohort = pd.concat([cohort, unit_dummies], axis=1)\n",
    "cohort = cohort.drop('unit_type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Save Train/Test Split of Features and Label\n",
    "\n",
    "We need to save the label and remove it from the features. We'll also need to save the APACHE prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = (cohort.hospital_expiration == 'EXPIRED').astype('int')\n",
    "apache_pred = cohort.apache_prediction\n",
    "cohort = cohort.drop(['hospital_expiration', 'apache_prediction'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can form a train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y, train_apache, test_apache = train_test_split(cohort, label, apache_pred, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we can save the CSV files corresponding to data frames we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train files\n",
    "train_X.to_csv('./data/train_X.csv', index=False)\n",
    "train_y.to_csv('./data/train_y.csv', index=False, header=True)\n",
    "train_apache.to_csv('./data/train_apache.csv', index=False, header=True)\n",
    "\n",
    "# test files\n",
    "test_X.to_csv('./data/test_X.csv', index=False)\n",
    "test_y.to_csv('./data/test_y.csv', index=False, header=True)\n",
    "test_apache.to_csv('./data/test_apache.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this portion complete, we can move onto the construction of our mortality models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
